{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #To hand with data \n",
    "import numpy as np #To math \n",
    "import seaborn as sns #to visualization\n",
    "import matplotlib.pyplot as plt # to plot the graphs\n",
    "import matplotlib.gridspec as gridspec # to do the grid of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "df_credit = pd.read_csv(\"../input/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking the how data looks\n",
    "df_credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is stardarized, I will explore them later\n",
    "#For now I will look the \"normal\" columns\n",
    "df_credit[[\"Time\",\"Amount\",\"Class\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets start looking the difference by Normal and Fraud transactions\n",
    "print(\"Distribuition of Normal(0) and Frauds(1): \")\n",
    "print(df_credit[\"Class\"].value_counts())\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.countplot(df_credit['Class'])\n",
    "plt.title(\"Class Count\", fontsize=18)\n",
    "plt.xlabel(\"Is fraud?\", fontsize=15)\n",
    "plt.ylabel(\"Count\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelta = pd.to_timedelta(df_credit['Time'], unit='s')\n",
    "df_credit['Time_min'] = (timedelta.dt.components.minutes).astype(int)\n",
    "df_credit['Time_hour'] = (timedelta.dt.components.hours).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the distribuition by Class types throught hours and minutes\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.distplot(df_credit[df_credit['Class'] == 0][\"Time_hour\"], \n",
    "             color='g')\n",
    "sns.distplot(df_credit[df_credit['Class'] == 1][\"Time_hour\"], \n",
    "             color='r')\n",
    "plt.title('Fraud x Normal Transactions by Hours', fontsize=17)\n",
    "plt.xlim([-1,25])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the distribuition by Class types throught hours and minutes\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.distplot(df_credit[df_credit['Class'] == 0][\"Time_min\"], \n",
    "             color='g')\n",
    "sns.distplot(df_credit[df_credit['Class'] == 1][\"Time_min\"], \n",
    "             color='r')\n",
    "plt.title('Fraud x Normal Transactions by minutes', fontsize=17)\n",
    "plt.xlim([-1,61])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To clearly the data of frauds and no frauds\n",
    "df_fraud = df_credit[df_credit['Class'] == 1]\n",
    "df_normal = df_credit[df_credit['Class'] == 0]\n",
    "\n",
    "print(\"Fraud transaction statistics\")\n",
    "print(df_fraud[\"Amount\"].describe())\n",
    "print(\"\\nNormal transaction statistics\")\n",
    "print(df_normal[\"Amount\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering to a better visualization of the values\n",
    "df_credit['Amount_log'] = np.log(df_credit.Amount + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "#I will explore the Amount by Class and see the distribuition of Amount transactions\n",
    "plt.subplot(121)\n",
    "ax = sns.boxplot(x =\"Class\",y=\"Amount\",\n",
    "                 data=df_credit)\n",
    "ax.set_title(\"Class x Amount\", fontsize=20)\n",
    "ax.set_xlabel(\"Is Fraud?\", fontsize=16)\n",
    "ax.set_ylabel(\"Amount(US)\", fontsize = 16)\n",
    "\n",
    "plt.subplot(122)\n",
    "ax1 = sns.boxplot(x =\"Class\",y=\"Amount_log\", data=df_credit)\n",
    "ax1.set_title(\"Class x Amount\", fontsize=20)\n",
    "ax1.set_xlabel(\"Is Fraud?\", fontsize=16)\n",
    "ax1.set_ylabel(\"Amount(Log)\", fontsize = 16)\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.6, top = 0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking the Amount and time distribuition of FRAUD transactions\n",
    "ax = sns.lmplot(y=\"Amount\", x=\"Time_min\", fit_reg=False,aspect=1.8,\n",
    "                data=df_credit, hue='Class')\n",
    "plt.title(\"Amounts by Minutes of Frauds and Normal Transactions\",fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lmplot(y=\"Amount\", x=\"Time_hour\", fit_reg=False,aspect=1.8,\n",
    "                data=df_credit, hue='Class')\n",
    "plt.title(\"Amounts by Hour of Frauds and Normal Transactions\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking the V's features\n",
    "columns = df_credit.iloc[:,1:29].columns\n",
    "\n",
    "frauds = df_credit.Class == 1\n",
    "normals = df_credit.Class == 0\n",
    "\n",
    "grid = gridspec.GridSpec(14, 2)\n",
    "plt.figure(figsize=(15,20*4))\n",
    "\n",
    "for n, col in enumerate(df_credit[columns]):\n",
    "    ax = plt.subplot(grid[n])\n",
    "    sns.distplot(df_credit[col][frauds], bins = 50, color='g') #Will receive the \"semi-salmon\" violin\n",
    "    sns.distplot(df_credit[col][normals], bins = 50, color='r') #Will receive the \"ocean\" color\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(str(col))\n",
    "    ax.set_xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will select the variables where fraud class have a interesting behavior and might can help us predict\n",
    "\n",
    "df_credit = df_credit[[\"Time_hour\",\"Time_min\",\"V2\",\"V3\",\"V4\",\"V9\",\"V10\",\"V11\",\"V12\",\"V14\",\"V16\",\"V17\",\"V18\",\"V19\",\"V27\",\"Amount\",\"Class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "df_credit.Amount = np.log(df_credit.Amount + 0.001)\n",
    "#Looking the final df\n",
    "df_credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.Greens\n",
    "\n",
    "plt.figure(figsize=(14,12))\n",
    "\n",
    "sns.heatmap(df_credit.corr(),linewidths=0.1,vmax=1.0, \n",
    "            square=True, cmap = colormap, linecolor='white', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb # To do our transformation in a unique time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, fbeta_score, confusion_matrix, precision_recall_curve, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_credit.drop([\"Class\"], axis=1).values #Setting the X to do the split\n",
    "y = df_credit[\"Class\"].values # transforming the values in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function that we will use to better evaluate the model\n",
    "def print_results(headline, true_value, pred):\n",
    "    print(headline)\n",
    "    print(\"accuracy: {}\".format(accuracy_score(true_value, pred)))\n",
    "    print(\"precision: {}\".format(precision_score(true_value, pred)))\n",
    "    print(\"recall: {}\".format(recall_score(true_value, pred)))\n",
    "    print(\"f2: {}\".format(fbeta_score(true_value, pred, beta=2)))\n",
    "\n",
    "# splitting data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, test_size=0.20)\n",
    "\n",
    "classifier = RandomForestClassifier\n",
    "\n",
    "# build model with SMOTE imblearn\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(random_state=4), \\\n",
    "                                   classifier(random_state=42))\n",
    "\n",
    "smote_model = smote_pipeline.fit(X_train, y_train)\n",
    "smote_prediction = smote_model.predict(X_test)\n",
    "\n",
    "#Showing the diference before and after the transformation used\n",
    "print(\"normal data distribution: {}\".format(Counter(y)))\n",
    "X_smote, y_smote = SMOTE().fit_sample(X, y)\n",
    "print(\"SMOTE data distribution: {}\".format(Counter(y_smote)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying random forest\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix(y_test, smote_prediction))\n",
    "\n",
    "print('\\nSMOTE Pipeline Score {}'.format(smote_pipeline.score(X_test, y_test)))\n",
    "\n",
    "print_results(\"\\nSMOTE + RandomForest classification\", y_test, smote_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = smote_pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Generate precision recall curve values: precision, recall, thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(precision, recall)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params of the model\n",
    "param_grid = {\"max_depth\": [3,5, None],\n",
    "              \"n_estimators\":[3,5,10],\n",
    "              \"max_features\": [5,6,7,8]}\n",
    "\n",
    "# Creating the classifier\n",
    "model = RandomForestClassifier(max_features=3, max_depth=2 ,n_estimators=10, random_state=3, criterion='entropy', n_jobs=1, verbose=1 )\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='recall')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the fit\n",
    "rf = RandomForestClassifier(max_depth=5, max_features = 7, n_estimators = 10)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the Training Score\n",
    "print(\"Training score data: \")\n",
    "print(rf.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the model \n",
    "#Predicting by X_test\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print_results(\"RF classification\", y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance plot\n",
    "features = [\"Time_min\", 'Time_hours',\"V2\",\"V3\",\"V4\",\"V9\",\"V10\",\"V11\",\"V12\",\"V14\",\"V16\",\"V17\",\"V18\",\"V19\",\"V27\",\"Amount\"]\n",
    "\n",
    "# Credits to Gabriel Preda\n",
    "# https://www.kaggle.com/gpreda/credit-card-fraud-detection-predictive-models\n",
    "plt.figure(figsize = (9,5))\n",
    "\n",
    "feat_import = pd.DataFrame({'Feature': features, 'Feature importance': rf.feature_importances_})\n",
    "feat_import = feat_import.sort_values(by='Feature importance',ascending=False)\n",
    "\n",
    "g = sns.barplot(x='Feature',y='Feature importance',data=feat_import)\n",
    "g.set_xticklabels(g.get_xticklabels(),rotation=90)\n",
    "g.set_title('Features importance - Random Forest',fontsize=20)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC random forest curve\n",
    "#Predicting proba\n",
    "y_pred_prob = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Generate precision recall curve values: precision, recall, thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(precision, recall)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(rf,X_train, y_train, cv=10, scoring='recall')\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "             'penalty':['l1', 'l2']}\n",
    "\n",
    "logreg = LogisticRegression(random_state=2)\n",
    "\n",
    "grid_search_lr = GridSearchCV(logreg, param_grid=param_grid, scoring='recall', cv=5)\n",
    "\n",
    "grid_search_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best recall obtained\n",
    "print(grid_search_lr.best_score_)\n",
    "#Best parameter on trainning set\n",
    "print(grid_search_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model \n",
    "logreg = LogisticRegression(C=10, penalty='l2',random_state=2)\n",
    "\n",
    "#Fiting the model\n",
    "logreg.fit(X_train, y_train)\n",
    "           \n",
    "# Printing the Training Score\n",
    "print(\"Cross Validation of X and y Train: \")\n",
    "print(cross_val_score(logreg,X_train, y_train, cv=5, scoring='recall'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with the best params\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\")\n",
    "print_results(\"LogReg classification\", y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting proba\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Generate precision recall curve values: precision, recall, thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(precision, recall)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
